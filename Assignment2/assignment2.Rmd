---
title: "CS 6301 Assignment 2"
author: "Daniel Crawford (dsc160130), Abhishek Thurlapati (vxt160630)"
output:
  html_document:
    df_print: paged
---

```{r}
#Grabs the cifar-10 dataset 
cifar_dir <- "cifar-10-batches-bin/"

if (!dir.exists(cifar_dir)) {
  download.file('http://www.cs.utoronto.ca/~kriz/cifar-10-binary.tar.gz', 'cifar-10.tar.gz')
  untar('cifar-10.tar.gz')
}
```

```{r}
#Automatically installs the ANN2 package for neural networks
if (!("ANN2" %in% installed.packages())) {
  install.packages("ANN2", dependencies = TRUE)
}
```

```{r}
library(ANN2)
```
It is hard to read a massive dataset like cifar10, so we will take portions of it instead for this part.

```{r}
batches_to_read <- 1
```

```{r}
# CIFAR-10: 32x32 images with 3 channels (RGB), 60000 rows in total.
cifar10 <- matrix(ncol = 32 * 32 * 3, nrow = 10000 * batches_to_read)
cifar10_labels <- rep(0, 10000 * batches_to_read)
```

```{r}
#Reads a small batch of the images from the dataset to work with locally
read_cifar_batch <- function (file) {
  f <- file(file, "rb")
  
  images <- matrix(ncol=32 * 32 * 3, nrow=10000)
  labels <- rep(0, 10000)
  for (i in 1:10000) {
    labels[i] <- readBin(f, "integer", size=1, n=1, endian="little", signed=FALSE)
    images[i, ] <- readBin(f, "integer", n = 32 * 32 * 3, size = 1, signed=FALSE, endian = "little")
  }
  close(f)
  
  list(
    images = images,
    labels = labels
  )
}
```

```{r}
#Reads in batch 1 images
for (i in 1:batches_to_read) {
  if (i == 6) {
    file <- paste(cifar_dir,"test_batch", ".bin", sep="")
  }
  else {
    file <- paste(cifar_dir, "data_batch_", i, ".bin", sep="")
  }
  print(paste("Reading ", file))
  batch <- read_cifar_batch(file)
  batch_range <- ((i - 1) * 10000 + 1):(i * 10000)
  cifar10[batch_range, ] <- batch$images
  cifar10_labels[batch_range] <- batch$labels
}
```

```{r}
#Grabs the labels of the images
label_names <- read.table("cifar-10-batches-bin/batches.meta.txt")
```

```{r}
#Creates a df of the cifar-10 data
df <- data.frame(cifar10, label=as.factor(cifar10_labels))
```

```{r}
#Normalizes the data
df[,1:(32 * 32 * 3)] <- df[,1:(32*32*3)] / 255

split_index = as.integer(nrow(df) * 0.8)
```

```{r}
#Splits the data into train and test
train <- df[1:split_index, ]
test <- df[split_index:nrow(df), ]
```

```{r}
#Displayes df
df[split_index:nrow(df),]
```

```{r}
#Grabs the columns of the predictors and the labels
feature_cols <- 1:(32*32*3)
label_col <- 32*32*3+1
```

```{r}
#Grabs the x and y of the training data
N <- nrow(train)
X <- train[1:N,feature_cols]
y <- train[1:N,label_col]
```

Despite using a deep learning library, I am not using any hidden layers to avoid taking advantage
of any deep learning capabilities. However, batching is useful because it helps handle a larger part
of the dataset.
```{r}
#Creates the neural network model
nn <- neuralnetwork(X, y, c(50), regression = FALSE, batch.size = 64, n.epochs = 20)
```

```{r}
#Grabs the predicted and true values of the dataset and calculates metrics
test_pred <- max.col(predict(nn, test[, feature_cols])$probabilities)
test_true <- test[,label_col]
mean(test_pred == as.integer(test_true))
```

Accuracy received: 37.9%

This means our model is making decent predictions, but it clearly lacks representational power.
