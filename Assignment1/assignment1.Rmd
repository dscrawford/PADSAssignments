---
title: "CS6301.003 Assignment 1"
author: "Daniel Crawford (dsc160130), Abhishek Thurlapati (vxt160630) , Pragya Karki (pxk170230)"
output:
  pdf_document: default
  html_notebook: default
---


```{r}
# install.packages(c('caret'))
```
Loading all the required packages for the program.
```{r}
#Loads packages
require(corrplot)
library(scales)
require(ggplot2)
library(caret)
```
Loading in the 'California Housing Dataset' we are working with.
```{r}
# Loads the dataset
df <- read.csv('https://raw.githubusercontent.com/dscrawford/PADSAssignments/main/Assignment1/housing.csv')
df[1:10, ]
```
Looking at the data, we know that we need to predict median_house_value using the other variable as predictors. We are given median values based on blocks of houses located at a certain (longitude,latitude). We are going to be creating a regression model and predicting a continuous variable. 

Displaying the number of null values and null value percentage of each variable in the dataset.
```{r}
sum(is.na(df))
# Percentage of null values
sapply(df, function(x) sum(is.na(x))) / nrow(df)
```
From the above information, we see that the only variable to have null values is total_bedrooms with 207 null values. Because we cannot estimate the total_bedrooms in a block based on the other blocks, we can handle the null values by eliminating them to allow our predictive model to run better. We do this because using the median, mean, or mode of the other blocks would would lead to false insights about this block of houses.
```{r}
# Remove unwanted null values
df <- na.omit(df)
```
Displaying the summary statistics of each variable in the dataset to get a better insight into the data
```{r}
# Shows summary statistics of the dataset
summary(df)
```
Now view histograms of each of the variables (Aside from the categorical variable 'ocean proximity').
```{r, message=FALSE, results='hide'}
# Displays the histograms of each variable
layout(matrix(1:9, 3, 3, byrow=TRUE))
lapply(colnames(df[-which(colnames(df) == 'ocean_proximity')]), 
       FUN=function(x) hist(as.vector(df[[x]]),
                            main=paste(x),
                            xlab=x,
                            breaks=50
                            )
       )
```
Above are the histograms of all the variables in the dataset that are potential predictors. We can see that most of these variables are normally distributed, but some have discrepancies. Some histograms are skewed. Total_rooms, total_bedrooms, populations, households, and median_income are all slightly right skewed. Housing_median_age seems to have a big jump in frequency of age at specific points in time. It is likely that something influenced more housing construction in these time periods, so it is an odd variable. Lastly, it looks like they capped the age at about 52 years old, likely because the data was not tracked before a certain time well. Longitude and Latitude being used is expected to be weird since they are location points. We can see though that the most densely populated areas will dominate our data set. Our target, median_house_value, has a cap similar to housing_median_age. These capped values may affect results.

We also want to see how useful the categorical variable Ocean Proximity is, so we are going to transform it into dummy variables. This allows for our model to work with this variable easier.
```{r}
# One hot encode categorical variables
dummy <- dummyVars(" ~ .", data=df)
df <- data.frame(predict(dummy, newdata=df))
df[1:3, 10:ncol(df)]
```
Now we create a correlation plot and display correlation values to determine the connection between each predictor to the predicted variable of median_house_value. 
```{r, message=FALSE, results='hide'}
#Create a correlation plot and display correlation values
M <- cor(df)
corplot <- corrplot(M, method="circle", tl.cex=0.5)
cormat <- as.data.frame(corrplot(M,method = "number", tl.cex=0.5, cl.cex = 0.5, number.cex= 0.6))
```
From the the data displayed above, we can see that many of these values are weakly correlated with the median house value, but we still see some linearity. Likely we will need to introduce a strategy to overcome these weak correlations.

Let us search for the variables with correlation above 0.5 to median_house_value.
```{r}
# Prints highly correlated values
row.names(cormat)[abs(cormat$median_house_value) > 0.5]
```
Only median_income has a high correlation with median_house_value other than itself. This variable will play a key factor in our predictive linear regression model. 

We have created some plots to further visualize the linearity of each variable to the median_house_value. 
```{r, message=FALSE, results='hide'}
#Plots to check linearity of each var
plot_line <- function (x) {
  plot(as.vector(df[[x]]),
       as.vector(df$median_house_value),
       main=paste(x),
       xlab=x,
       ylab='median_house_value')
  abline(lm(df$median_house_value ~ df[[x]]), lwd=3, col="red")
}

layout(matrix(c(1:col(df)), 7, 2, byrow=TRUE), widths=rep(1, ncol(df)), heights=rep(1.5, ncol(df)))
lapply(colnames(df[-which(colnames(df) == 'median_house_value')]), 
       FUN=plot_line
) 
```
Although many of the variables share a linear relationship with median_house_value, a few outliers and distribution of points lessen the linearity. We saw this with the low correlation values presented earlier, but now we are able to see the distribution of the points and determine the reasoning for this issue. Even through these issues, we can see the general linear relationship among the variables and median_house_value. Therefore we can say that creating a linear model with the 'lm()' function is the right choice. 

Below we attempt to remove some outliers at the risk of reducing the number of data entries to ensure better linearity with the variables. 
```{r, message=FALSE, results='hide'}
# Cleanup data by removing some outliers
df2 <-  subset(df, population < 25000)
df2 <-  subset(df2, total_rooms < 30000)
df2 <-  subset(df2, total_bedrooms < 6000)
df2 <-  subset(df2, households < 6000)

#Checks to make sure they are deleted
summary(df2)

#Plots to check linearity of each var
plot_line <- function (x) {
  plot(as.vector(df2[[x]]),
       as.vector(df2$median_house_value),
       main=paste(x),
       xlab=x,
       ylab='median_house_value')
  abline(lm(df2$median_house_value ~ df2[[x]]), lwd=3, col="red")
}

layout(matrix(c(1:col(df2)), 7, 2, byrow=TRUE), widths=rep(1, ncol(df2)), heights=rep(1.5, ncol(df2)))
lapply(colnames(df2[-which(colnames(df2) == 'median_house_value')]), 
       FUN=plot_line
) 

# Finalizes the new dataset with removed outliers
df <- df2
```
After removing some outliers in population, total_rooms, total_bedrooms, and households, we can see that the linearity has become more prominent and the distributions look a little better.

We want to look into geographical data, since location is very important to housing value. Hence, we start plotting our coordinates for each entry where the **color** feature are varied between *median_house_value*, *median_income*, and *households* while maintaining **size** as *population*. 
```{r}
mhv_map = ggplot(df, aes(x = longitude, y = latitude, color = median_house_value)) +
  geom_point(aes(size = population), alpha = 1) +
  xlab("Longitude") +
  ylab("Latitude") +
  ggtitle("Median House Value with Population") +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_color_distiller(palette = "Spectral", labels = comma) +
  labs(color = "Median House Value (in $USD)", size = "Population")
mhv_map
```
From the above plot, we can see that the coastal properties i.e., the ones with close ocean proximity have higher median house value than inland properties. As we move inland, property values gradually drop below $USD 200,000.
```{r}
mi_map = ggplot(df, aes(x = longitude, y = latitude, color = median_income*10000)) +
  geom_point(aes(size = population), alpha = 1) +
  xlab("Longitude") +
  ylab("Latitude") +
  ggtitle("Median Income with Population") +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_color_distiller(palette = "Spectral", labels = comma) +
  labs(color = "Median Income (in $USD)", size = "Population")
mi_map
```
From the above plot, we can observe that most households in California have a median income of $USD 50,000 or less. We also notice that the coastal region tends to have above average median income, which gradually declines as we move inland. This is similar to median house value, which we observed in previous plot. 

We also observe how population density in coastal region affects the median income of this region. Densely populated coastal regions tend to have higher median income than the sparsely populated ones.
```{r}
hs_map = ggplot(df, aes(x = longitude, y = latitude, color = households)) +
  geom_point(aes(size = population), alpha = 1) +
  xlab("Longitude") +
  ylab("Latitude") +
  ggtitle("Number of Households with Population") +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_color_distiller(palette = "Spectral", labels = comma) +
  labs(color = "Number of Households", size = "Population")
hs_map
```
From the above plot, we can see that densely populated areas tend have higher number of households. We also notice that number of households are far greater in and around coastal regions, where each region contains more than a 1,000 households.

Based on these three plots, we can observe the role played by population of a region when it comes to determining median house value, median income, and number of households for that region. In most cases, we have observed a positive correlation between *population* and *median_house_value*, *median_income*, and *households*.
```{r}
mhi_map = ggplot(df, aes(x = longitude, y = latitude, color = median_house_value)) +
  geom_point(aes(size = median_income*10000), alpha = 1) +
  xlab("Longitude") +
  ylab("Latitude") +
  ggtitle("Median House Value with Median Income") +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_color_distiller(palette = "Spectral", labels = comma) +
  labs(color = "Median Hosue Value (in $USD)", size = "Median Income (in $USD)")
mhi_map
```
This final plot helps illustrate the high correlation between *median_house_value* and *median_income* based on our preliminary analysis of *housing* dataset.

Split data in train/test split to test for over fitting. Doing this also allows us to test our model for general error metrics. 
```{r}
# Train/test split
set.seed(123)
sample <- sample.int(n = nrow(df), size = floor(0.75 * nrow(df)), replace=F)
train <- df[sample, ]
test <- df[-sample, ]
```
We decided we are ready to create models to predict the median_house_value. We first set up a base model with only the highly correlated predictors, and in our case the only one is median_income. 
```{r}
#Create base model with median_income
fit1 <- lm(median_house_value ~ median_income, data = train)

#Display model summary
summary(fit1)
```
Our first model with only highly correlated values yielded a adjusted r^2 of .4756. This value measures the goodness of the model fit. We will want to see if adding other variables can improve the adjusted r^2 and help improve the model. In addition, we can also see that the median_income is a significant predictor with a high t-val and a low p-val. The large range in residuals indicate errors are present. 

Now we test to see if adding all the predictors will improve our model. 
```{r}
# Create model with all predictors
fit2 <- lm(median_house_value ~ ., train)

#Display model summary
summary(fit2)
```
We can see that the adjusted r^2 in the second model is much higher than our first model with .652. This tells us that even the added predictors were weakly correlated, they helped improve our regression model. All of them except ocean_proximity.1H.OCEAN had 3 significance stars. Our model seems to be improving. 

We create a third model with the 2-way interactions between the variables to determine if any combination of predictors can help improve our model. With these 2-day interactions, we can also account for longitude and latitude in a multiplicative way. Unfortunately, this can have issues, (EX: 6\*2 = 12 and 1 \* 12 = 12). Regardless, this can still help pinpoint location.

One final observation: ocean_proximityNEAR.OCEAN is not useful, so we will remove it.
```{r}
# Removes unnecessary variable
train = train[which(colnames(train) != "ocean_proximityNEAR.OCEAN")]

# Create model with all 2-way interactions
fit3 <- lm(median_house_value ~ (.)^2, data = train)

#Display model summary
summary(fit3)
```
In our third model, the adjusted r^2 goes up again to .7026. Some of the interactions we created were significant to the model, but some have little significance. This model seems to be better than our second model. We can try to clean this model up further and remove the less significant interactions.

Another observation we made was that there were 4 very highly correlated variables: total_rooms, total_bedrooms, population, households. In terms of correlation, total_rooms has the highest value so we should eliminate the other 3 to prevent multicollinearity.

Finally, while the summary is showing us NAN values for some of the interactions, R will handle this for us. So these interactions will simply not be used or need to be specially handled.
```{r}
# Removes variables to prevent multicollinearity problems
rm_variables <- c("total_rooms", "total_bedrooms", "households")
'%!in%' <- function(x,y)!('%in%'(x,y))

# Create model with interactions cleaned up
fit4 <- lm(median_house_value ~ (.)^2, data = train[,which(colnames(train) %!in% rm_variables)])

#Display model summary
summary(fit4)
```
As we can see from this 4th model, removing the highly correlated variables seemed to only hurt our performance and lower adjusted r^2 to .6425 rather than benefit it. Therefore, we can justify keeping the variables in and sticking to fit3.

In this model, we want to focus more on just improving on location data. Therefore, we are going to apply a poly to the product of longitude and latitude to see if the model can understand the variables better.

```{r, message=FALSE, results='hide'}
# Create model with interactions cleaned up
fit5 <- lm(median_house_value ~ poly(longitude * latitude, 5)[,2:5] + (.)^2, data = train)

#Display model summary
summary(fit5)

#Test the confidence of the model
confint(fit5, level = 0.90)

#Plot the model
plot(fit5)
```
In our final model, we can see that adding more ability for the model to utilize longitude and latitude seemed to help a lot. We improved to an adjusted R-squared of 0.7129. Ultimately, what we can see from this model is that we need more representational power or more data entries to learn from, and likely a linear model is not quite powerful enough. As we keep adding different interpretations of the data, we see more improvements.
