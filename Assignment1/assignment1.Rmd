---
title: "CS6301.003 Assignment 1"
author: "Daniel Crawford, Abhishek Thurlapati, Pragya Karki"
output: html_notebook
---


```{r}
# install.packages(c('caret'))
```
Loading all the required packages for the program
```{r}
#Loads packages
require(corrplot)
library(scales)
require(ggplot2)
library(caret)
```
Loading in the 'California Housing Dataset' we are working with 
```{r}
# Loads the dataset
df <- read.csv('housing.csv')
df[1:10, ]
```
Looking at the data, we know that we need to predict median_house_value using the other variable as predictors. We are given median values based on blocks of houses located at a certain (longitude,latitude). We are going to be creating a regression model and predicting a continuous variable. 

Displaying the number of null values and null value percentage of each variable in the dataset
```{r}
# Percentage of null values
sum(is.na(df))
sapply(df, function(x) sum(is.na(x))) / nrow(df)

```
From the above information, we see that the only variable to have null values is total_bedrooms with 207 null values. Because we cannot estimate the total_bedrooms in a block based on the other blocks, we can handle the null values by eliminating them to allow our predictive model to run better. We do this because, using the median, mean, or mode of the other blocks would would lead to false insights about this block of houses.
```{r}
# Remove unwanted null values
df <- na.omit(df)
```
Displaying the summary statistics of each variable in the dataset
```{r}
# Shows summary statistics of the dataset
summary(df)
```
Now view histograms of each of the variables (Aside from the categorical variable 'ocean proximity').
```{r, message=FALSE, results='hide'}
layout(matrix(1:9, 3, 3, byrow=TRUE))
lapply(colnames(df[-which(colnames(df) == 'ocean_proximity')]), 
       FUN=function(x) hist(as.vector(df[[x]]),
                            main=paste(x),
                            xlab=x,
                            breaks=50
                            )
       )
```
 (didn't touch).Above is the histograms of all the variables available to us. We can see that most of these variables are normally distributed, but some have discrepancies. Some histograms are skewed. 
Housing median age seems to have a big jump in frequency of age at specific points in time. It is likely that something influenced more housing construction in these time periods, so it is an odd variable. Lastly, it looks like they capped the age at about 52 years old, likely because the data was not tracked before a certain time well.
Longitude and Latitude being used is expected to be weird since they are location points. We can see though that the most densely populated areas will dominate our data set.
Our target, median house value, has a cap similar to housing median age. Because of this, these outliers can hurt our results pretty badly on a linear fit.

We also want to see how useful the categorical variable Ocean Proximity is, so we are going to transform it into dummy variables. This allows for our model to work with this variable easier.
```{r}
# One hot encode categorical variables
dummy <- dummyVars(" ~ .", data=df)
df <- data.frame(predict(dummy, newdata=df))
df[1:3, 10:ncol(df)]
```
Now we create a correlation plot and display correlation values to determine the connection between each predictor to the predicted variable of median_house_value. 
```{r}
#Create a correlation plot and display correlation values
M <- cor(df)
corplot <- corrplot(M, method="circle", tl.cex=0.5)
cormat <- as.data.frame(corrplot(M,method = "number", tl.cex=0.5, cl.cex = 0.5, number.cex= 0.6))
M
```
From the the data displayed above, we can see that many of these values are weakly correlated with the median house value, but we still see some linearity. Likely we will need to introduce a strategy to overcome these weak correlations.

Let us search for the variables with correlation above 0.5 to median_house_value.
```{r}
row.names(cormat)[abs(cormat$median_house_value) > 0.5]
```
Only 'median_income' has a correlation with median_house_value other than itself. This variable will play a key factor in our predictive linear regression model. 

We have created some plots to further visualize the linearity of each variable to the median_house_value. 
```{r}
#Plots to check linearity of each var

plot_line <- function (x) {
  plot(as.vector(df[[x]]),
       as.vector(df$median_house_value),
       main=paste(x),
       xlab=x,
       ylab='median_house_value')
  abline(lm(df$median_house_value ~ df[[x]]), lwd=3, col="red")
}

layout(matrix(c(1:col(df)), 7, 2, byrow=TRUE), widths=rep(1, ncol(df)), heights=rep(1.5, ncol(df)))
lapply(colnames(df[-which(colnames(df) == 'median_house_value')]), 
       FUN=plot_line
) 
```
We want to look into geographical data, since location is very important to housing value.

```{r}
# Geographical map
mhv_map = ggplot(train, aes(x = longitude, y = latitude, color = median_house_value)) +
  geom_point(aes(size = population), alpha = 1) +
  xlab("Longitude") +
  ylab("Latitude") +
  ggtitle("Median House Value wrt Population Map") +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_color_distiller(palette = "Spectral", labels = comma) +
  labs(color = "Median House Value (in $USD)", size = "Population")
mhv_map
```

```{r}
mi_map = ggplot(train, aes(x = longitude, y = latitude, color = median_income)) +
  geom_point(aes(size = population), alpha = 1) +
  xlab("Longitude") +
  ylab("Latitude") +
  ggtitle("Median Income wrt Population Map") +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_color_distiller(palette = "Spectral", labels = comma) +
  labs(color = "Median Income (in tens of thousands of $USD)", size = "Population")
mi_map
```

```{r}
hs_map = ggplot(train, aes(x = longitude, y = latitude, color = households)) +
  geom_point(aes(size = population), alpha = 1) +
  xlab("Longitude") +
  ylab("Latitude") +
  ggtitle("Total Number of Households wrt Population Map") +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_color_distiller(palette = "Spectral", labels = comma) +
  labs(color = "Total Number of Households", size = "Population")
hs_map
```
```{r}
library(plotly)

fig <- plot_ly(x = df$longitude, y = df$latitude, z = df$households)
```

```{r}
fig %>% add_histogram2d(xbins=100, ybins=100, nbinsx=100, nbinsy=100, colorscale='Hot', histfunc='sum')
```



Split data in train/test split to test for over fitting.
```{r}
# Train/test split
sample <- sample.int(n = nrow(df), size = floor(0.75 * nrow(df)), replace=F)
train <- df[sample, ]
test <- df[-sample, ]
```

```{r}
# Starting fits based on information
# First first based on median income
lm.fit1 = lm(median_house_value~median_income, data=train)
summary(lm.fit1)
```


```{r}
fit1 <- lm(train$median_house_value ~ train$median_income, train)
summary(fit1)
confint(fit1, level = 0.90)
```


```{r}
plot(fit1)
```

```{r}
# Next model (ocean proximity)
fit2 <- lm(median_house_value ~ ., train)
summary(fit2)
confint(fit2, level = 0.90)
```
```{r}
fit3 <- lm(median_house_value ~ (.)^2, train)
summary(fit3)
```
```{r}
plot(fit3)
```

