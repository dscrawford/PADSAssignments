---
title: "CS6301.003 Assignment 1"
author: "Daniel Crawford, Abhishek Thurlapati, Pragya Karki"
output:
  html_document:
    df_print: paged
---


```{r}
# install.packages(c('caret'))
```
Loading all the required packages for the program
```{r}
#Loads packages
require(corrplot)
library(scales)
require(ggplot2)
library(caret)
```
Loading in the 'California Housing Dataset' we are working with 
```{r}
# Loads the dataset
df <- read.csv('housing.csv')
df[1:10, ]
```
Looking at the data, we know that we need to predict median_house_value using the other variable as predictors. We are given median values based on blocks of houses located at a certain (longitude,latitude). We are going to be creating a regression model and predicting a continuous variable. 

Displaying the number of null values and null value percentage of each variable in the dataset
```{r}
# Percentage of null values
sum(is.na(df))
sapply(df, function(x) sum(is.na(x))) / nrow(df)

```
From the above information, we see that the only variable to have null values is total_bedrooms with 207 null values. Because we cannot estimate the total_bedrooms in a block based on the other blocks, we can handle the null values by eliminating them to allow our predictive model to run better. We do this because, using the median, mean, or mode of the other blocks would would lead to false insights about this block of houses.
```{r}
# Remove unwanted null values
df <- na.omit(df)

```
Displaying the summary statistics of each variable in the dataset to get a better insight into the data
```{r}
# Shows summary statistics of the dataset
summary(df)

```
Now view histograms of each of the variables (Aside from the categorical variable 'ocean proximity').
```{r, message=FALSE, results='hide'}
# Displays the histograms of each variable
layout(matrix(1:9, 3, 3, byrow=TRUE))
lapply(colnames(df[-which(colnames(df) == 'ocean_proximity')]), 
       FUN=function(x) hist(as.vector(df[[x]]),
                            main=paste(x),
                            xlab=x,
                            breaks=50
                            )
       )
```
 (didn't touch).Above is the histograms of all the variables available to us. We can see that most of these variables are normally distributed, but some have discrepancies. Some histograms are skewed. 
Housing median age seems to have a big jump in frequency of age at specific points in time. It is likely that something influenced more housing construction in these time periods, so it is an odd variable. Lastly, it looks like they capped the age at about 52 years old, likely because the data was not tracked before a certain time well.
Longitude and Latitude being used is expected to be weird since they are location points. We can see though that the most densely populated areas will dominate our data set.
Our target, median house value, has a cap similar to housing median age. Because of this, these outliers can hurt our results pretty badly on a linear fit.

We also want to see how useful the categorical variable Ocean Proximity is, so we are going to transform it into dummy variables. This allows for our model to work with this variable easier.
```{r}
# One hot encode categorical variables
dummy <- dummyVars(" ~ .", data=df)
df <- data.frame(predict(dummy, newdata=df))
df[1:3, 10:ncol(df)]
```
Now we create a correlation plot and display correlation values to determine the connection between each predictor to the predicted variable of median_house_value. 
```{r}
#Create a correlation plot and display correlation values
M <- cor(df)
corplot <- corrplot(M, method="circle", tl.cex=0.5)
cormat <- as.data.frame(corrplot(M,method = "number", tl.cex=0.5, cl.cex = 0.5, number.cex= 0.6))

```
From the the data displayed above, we can see that many of these values are weakly correlated with the median house value, but we still see some linearity. Likely we will need to introduce a strategy to overcome these weak correlations.

Let us search for the variables with correlation above 0.5 to median_house_value.
```{r}
# Prints highly correlated values
row.names(cormat)[abs(cormat$median_house_value) > 0.5]
```
Only median_income has a correlation with median_house_value other than itself. This variable will play a key factor in our predictive linear regression model. 

We have created some plots to further visualize the linearity of each variable to the median_house_value. 
```{r}
#Plots to check linearity of each var
plot_line <- function (x) {
  plot(as.vector(df[[x]]),
       as.vector(df$median_house_value),
       main=paste(x),
       xlab=x,
       ylab='median_house_value')
  abline(lm(df$median_house_value ~ df[[x]]), lwd=3, col="red")
}

layout(matrix(c(1:col(df)), 7, 2, byrow=TRUE), widths=rep(1, ncol(df)), heights=rep(1.5, ncol(df)))
lapply(colnames(df[-which(colnames(df) == 'median_house_value')]), 
       FUN=plot_line
) 
```
(Review this analysis) Although many of the variables share a linear relationship with median_house_value, the few outliers and distribution of points lessen the linearity. We saw this with the low correlation values presented earlier, but now we are able to see the distribution of the points and determine the reasoning for this issue. Even through these issues, we can see the general linear relationship among the variables and median_house_value. Therefore we can say that creating a linear model with the 'lm' function is the right choice. 

Below we attempt to remove some outliers at the risk of reduce the number of data entries to ensure better linearity with the variables. 
```{r}
# Cleanup data by removing some outliers
df2 <-  subset(df, population < 25000)
df2 <-  subset(df2, total_rooms < 30000)
df2 <-  subset(df2, total_bedrooms < 6000)
df2 <-  subset(df2, households < 6000)

#Checks to make sure they are deleted
summary(df2)

#Plots to check linearity of each var
plot_line <- function (x) {
  plot(as.vector(df2[[x]]),
       as.vector(df2$median_house_value),
       main=paste(x),
       xlab=x,
       ylab='median_house_value')
  abline(lm(df2$median_house_value ~ df2[[x]]), lwd=3, col="red")
}

layout(matrix(c(1:col(df2)), 7, 2, byrow=TRUE), widths=rep(1, ncol(df2)), heights=rep(1.5, ncol(df2)))
lapply(colnames(df2[-which(colnames(df2) == 'median_house_value')]), 
       FUN=plot_line
) 
```
We want to look into geographical data, since location is very important to housing value.

houses close to ocean have above average value
```{r}
# Geographical map
mhv_map = ggplot(df2, aes(x = longitude, y = latitude, color = median_house_value)) +
  geom_point(aes(size = population), alpha = 1) +
  xlab("Longitude") +
  ylab("Latitude") +
  ggtitle("Median House Value wrt Population Map") +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_color_distiller(palette = "Spectral", labels = comma) +
  labs(color = "Median House Value (in $USD)", size = "Population")
mhv_map
```
(We may need some more analysis on the plot?)
median_income is above average in properties near ocean fronts.
```{r}
mi_map = ggplot(df2, aes(x = longitude, y = latitude, color = median_income)) +
  geom_point(aes(size = population), alpha = 1) +
  xlab("Longitude") +
  ylab("Latitude") +
  ggtitle("Median Income wrt Population Map") +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_color_distiller(palette = "Spectral", labels = comma) +
  labs(color = "Median Income (in tens of thousands of $USD)", size = "Population")
mi_map
```
(We may need some more analysis on the plot?)
coastal region is densely populated based on the number of household, which does help contribute to the house_value
```{r}
hs_map = ggplot(df2, aes(x = longitude, y = latitude, color = households)) +
  geom_point(aes(size = population), alpha = 1) +
  xlab("Longitude") +
  ylab("Latitude") +
  ggtitle("Total Number of Households wrt Population Map") +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_color_distiller(palette = "Spectral", labels = comma) +
  labs(color = "Total Number of Households", size = "Population")
hs_map
```
(We may need some more analysis on the plot?)

Split data in train/test split to test for over fitting. Doing this also allows us to test our model for general accuracy error percentage. 
```{r}
# Train/test split
set.seed(123)
sample <- sample.int(n = nrow(df2), size = floor(0.75 * nrow(df2)), replace=F)
train <- df2[sample, ]
test <- df2[-sample, ]
```
LINEAR MODELS

We decided we are ready to create models to predict the median_house_value. We first set up a base model with only the highly correlated predictors, and in our case the only one is median_income. 
```{r}
#Create base model with median_income
fit1 <- lm(median_house_value ~ median_income, data = train)

#Display model summary
summary(fit1)

#Test the confidence of the model
confint(fit1, level = 0.90)

#Plot the model
plot(fit1)

```
(Review this analysis) Our first model with only highly correlated values yielded a adjusted r^2 of .4764. This value measures the goodness of the model. We will want to see if adding other variables can improve the adjusted r^2 and help improve the model. In addition, we can also see that the median_income is a significant predictor with a high t-val and a low p-val. The large range in residuals indicate errors are present. 

Now we test to see if adding all the predictors will improve our model. 
```{r}
# Create model with all predictors
fit2 <- lm(median_house_value ~ ., train)

# Display model summary
summary(fit2)

# Test the confidence of the model
confint(fit2, level = 0.90)

# Plot the model
plot(fit2)
```
(Review this analysis) We can see that the adjusted r^2 in the second model is much higher than our first model with .646. This tells us that even the added predictors were weakly correlated, they helped improve our regression model. All of them except ocean_proximity.1H.OCEAN had 3 significance stars. Our model seems to be improving. 

We create a third model with the 2-way interactions between the variables to determine if any combination of predictors can help improve our model. 
```{r}
# Create model with all 2-way interactions
fit3 <- lm(median_house_value ~ (.)^2, data = train)

# Display model summary
summary(fit3)

# Test the confidence of the model
confint(fit3, level = 0.90)

# Plot the model
plot(fit3)

```
In our third model, the adjusted r^2 goes up again to .7016. Some of the interactions we created were significant to the model, but some have little significance. This model seems to be better than our second model.

We can try to clean this model up further and remove the less significant interactions. 
```{r}
# Create model with interactions cleaned up
fit4 <- lm(median_house_value ~ (.)^2 - population - households - longitude:population - longitude:households - latitude:households - total_rooms:total_bedrooms - total_bedrooms:median_income - population:households - households:median_income, data = train)

# Display model summary
summary(fit4)

# Test the confidence of the model
confint(fit4, level = 0.90)

# Plot the model
plot(fit4)
```
(ANALYSIS OF 4th model)